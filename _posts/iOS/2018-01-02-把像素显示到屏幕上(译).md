---
layout: article
title: "把像素显示到屏幕上(译)"
categories: iOS
excerpt: "如何把一个像素显示到屏幕上？有很多种方法，这些方法包含不同的framework和很多不同的函数和方法的组合。在这里，我们将讲述一些发生在幕后的事情。"
ads: true
share: false
image:
---

**By [Daniel Eggert](https://twitter.com/danielboedewadt)** 

[原文网址](https://www.objc.io/issues/3-views/moving-pixels-onto-the-screen/)

如何把一个像素显示到屏幕上？有很多种方法，这些方法包含不同的framework和很多不同的函数和方法的组合。在这里，我们将讲述一些发生在幕后的事情。我们希望这篇文章能帮你理解当你需要决定调试和修复性能问题的时候哪个API最好。这篇文章虽然是以iOS为例，但是大多数讲的内容也同时适用于OS X。

## 图形堆栈

当像素要显示到屏幕上时，许多事情在幕后悄悄进行着。但是一旦这些像素已经在屏幕上了，每个像素由三个元素组成：红、黄、蓝。三种单独的颜色单元以不同的强度叠加显示，组成一个特定颜色的像素。你的iPhone5，液晶显示屏有1,136*640 = 727,040个像素，也就是2,181,120个颜色单元。一个15英寸的retina屏幕的话，这个数字将会超过15500000。整儿图形堆栈一起动作来确保每个颜色单元显示正确的强度。当你滚动屏幕时，所有的这些鼠疫百万计的像素每秒要更新60次，这是一个巨大的工作量。

### 软件部分

简单的看，软件上的栈看起来就像这样：

![](https://www.objc.io/images/issue-3/pixels-software-stack@2x-1ae69f5a.png)

Display的上面是GPU（graphics processing unit）。GPU是一个专门为并行图形计算设计的高并发的处理单元。这样才能更新大量的像素点，并把他们显示在屏幕上。它的并行特性也使得它可以很有效地合成纹理。我们一会儿会更详细的讨论合成纹理。关键是GPU是专门设计的，因此在处理一些类似工作时更加高效，例如它处理图像时比cpu更快，消耗的能量更少。CPU有着广泛的用途，可以做很多不同的事情，但是例如合成纹理的话，还是GPU更快。

GPU驱动是一段可以直接和GPU对话的代码。它让不同类型的GPU对下一层OpenGL/OpenGL ES表现的更加统一。

OpenGL(Open Graphics Libiary) 是一个渲染2D或者3D图像的API。GPU是一个非常专业化的硬件，OpenGL和GPU非常紧密的合作来发挥GPU的计算特性和实现硬件加速渲染。对于很多人来说，OpenGL或许看起来非常底层，但是它在1992年发布时，就是第一个和GPU对话的主要的标准方法，这是一个巨大的进步，因为程序员再也不用为了各个GPU重写他们的应用。

在OpenGL上一层，有点儿分叉。在iOS系统上，几乎所有的事情都是通过Core Animation，在OS X上，Core Graphics 绕过Core Animation的情况非常常见。对于一些专门化的应用来说，尤其是游戏，app 可以直接和OpenGL/OpenGL ES对话。有点儿让人有点儿迷惑的是Core Animation 的部分渲染使用了 Core Graphics，像AVFoundation、Core Image和其他框架这样的框架可以将所有东西混合起来。

要记住一件事情，GPU是一个非常强大的进行图像处理的硬件，在显示像素上处于中心地位。它和CPU连接。在硬件层面，总线连接GPU和CPU，像OpenGL, Core Animation和 Core Graphics这些框架协调GPU和CPU之间的数据传输。为了让像素点显示在屏幕上，有些处理需要在CPU上进行，然后处理过的数据传输给GPU，GPU再做处理，最终像素点显示在屏幕上。

这个过程的每个部分都有自己的挑战的部分，在这个过程中也会有一些权衡。

### 硬件部分

![](https://www.objc.io/images/issue-3/pixels,%20hardware@2x-861825d9.png)

一个非常简单的挑战看起来像这样：GPU有一些由每一帧合成的纹理(位图)，每个纹理都占用VRAM(Vedio RAM)，因此GPU可以保留多少纹理是有限制的。即使GPU在合成纹理方面非常高效，但是总有一些更加复杂的合成任务，而GPU在16.7ms(1/60s)的时间内处理多少工作也是有限制的，不可能是无限的。

另一个挑战是把要处理的数据给到GPU。为了能让GPU访问到数据，需要把数据从RAM转移到VRAM里面，也就是把数据上传到GPU。这似乎微不足道，单对于大量要处理的纹理来说就会是非常耗时的操作。

最后，你的应用程序是在CPU上跑的，你会告诉CPU从你的bundle文件中加载一个PNG图片，并解压它，这些都发生在CPU上。当你想要显示解压的图片时，就需要上传到GPU。像显示文本这样平凡的事情对于CPU来说是非常复杂的任务，它要使Core Text 和 Core Graphics框架紧密结合来生成文本的位图，然后以纹理的格式上传给GPU去显示。当你在屏幕上拖动或移动这个文本，这个纹理可以重用，CPU只需要告诉GPU新的位置就可以了，因此GPU可以重用已经存在的纹理。CPU不需要重新渲染文本，也不需要重新上传位图。

以上说明了其中的一些复杂性。了解了这部分概述，我们将深入探讨其中的一些技术。

## 合成纹理

在图形世界中`合成`是一个术语，用来描述不同的位图是如何组合在一起来创建你在屏幕上看到的最终图像。从很多方面来说，显而易见，让你很容易忘记涉及的复杂性和计算。

让我们忽略一些更深奥的案例，假定所有屏幕上的内容都是纹理。纹理是一个RGBA值的矩形区域，也就是说，对于每个像素都有红、绿、蓝颜色值和透明度，在Core Animation里，这叫做`CALayer`。

在这个稍微简化的设置下，每个layer都是一个纹理，所有的这些纹理都是以某种方式相互叠加。对于屏幕上的每个像素来说，GPU需要弄清楚如何混合这些纹理得到这个像素最终的RGB值，这就是`合成`。

如果我们在屏幕上只有一个单一的纹理，并且和屏幕大小相等，每个屏幕上的像素对应纹理上的像素，纹理的像素最终成为屏幕的像素。

如果我们有第二个纹理放在第一个纹理的上面，GPU就不得不合成这个纹理到第一个上。我们假定两个纹理是像素对齐的，并且使用正常的混合模式，用下面的公式计算每个像素的颜色：

```
R = S + D * (1 - Sa)
```

结果颜色Result等于Source颜色(上层)加Destination颜色(下层)乘以1减去Source颜色的alpha值。

很明显，这里发生了很多事情，让我们再假定一下所有的纹理都是不透明的，也即是alpha=1，如果destination纹理是蓝色（RGB = 0，0，1），source纹理是红色（RGB = 1，0，0），因为Sa=1，所以`R = S`,结果颜色是红色，和你的期望值相符。

如果Source（上层）layer的透明度是50%，S的RGB值将会是（0.5，0，0），公式将会变成这样：

```
                       0.5   0               0.5
R = S + D * (1 - Sa) = 0   + 0 * (1 - 0.5) = 0
                       0     1               0.5
```

我们最终得到了一个RGB值(0.5,0,0.5)，一个饱和的紫色，这是你在把透明红色和蓝色背景混合在一起时的第一感觉。

目前为止我们只计算了一个纹理的单个像素和另一个纹理的单个像素的合成，而GPU则需要计算所有纹理的所有像素合成。正如你知道的，大多数的app有很多的layers，所有的纹理都需要合成到一起，即使GPU是一个高度优化的专门做这种事情的硬件，这也让GPU变得很忙。

### 不透明 VS. 透明度(Opaque vs. Transparent)

当Source纹理完全不透明时，生成的像素和Source纹理一样，这会节省GPU很多工作，因为它可以简单地复制Source纹理的像素值。但是，GPU无法分辨纹理中是否有像素不透明，只有作为程序员的你知道你向`CALayer`里放了什么。这就是`CALayer`有一个叫做`opaque`属性的原因，如果`opaque`是`YES`,GPU将会忽略下面的层直接从这个layer复制纹理，这节省了GPU大量的工作。在Instruments选项中`color blended layers`让你可以看到那些layers被标记为非不透明的，这样的layer需要混合。合成不透明layer消耗更少，因为计算的比较少。

因此，如果你知道你的layer是不透明的，把`opaque`设置成`YES`。如果你要加载一个没有alpha通道的图片然后显示到`UIImageView`，这是一个自动的过程。注意，一个没有alpha通道的图片和一个alpha为100%的图片有着巨大的差异，后者情况下，Core Animation需要假定或许有一些像素的alpha值不是100%，这就需要检验和计算。在`Finder`中，你可以通过`显示简介`中的`更多信息`中的`alpha通道`来判断图片是否包含alpha通道。

### 像素对齐和不对齐

到目前为止，我们已经研究了那些像素对齐的layers的显示，当所有的layer都是像素对齐的时候我们只需要相对简单的数学运算。无论何时GPU都要弄清楚屏幕上的像素应该是什么颜色，它只需要看在屏幕上的所有layers的像素，并合成他们，或者当最上层的layer是不透明时，只需要复制最上层的layer的纹理。

我们说一个layer是像素对齐的是指layer上所有的像素都和屏幕上的像素完美对齐。有以下两个原因会导致不是这种像素对齐的情况：第一、缩放，当一个纹理放大或者缩小的时候，纹理的像素点就无法和屏幕的像素点对齐；***第二、纹理的来源不是像素边界***。

在这两种情况下，GPU需要做额外的数学计算。它要把多个像素点混合计算出一个值用来合成。当像素对齐的时候，GPU做的工作就少很多。

再次，`Core Animation Instrument`有一个叫`color misaligned images`的选项，当像素不对齐时会告诉你是哪个layer实例。

### 蒙层(Mask)

一个layer有一个与之关联的蒙层，这个蒙层是一个在合成到下层layer之前应用到layer的像素上的alpha值的位图。当你设置layer的圆角半径时，实际上你是在那个layer上设置了一个蒙层。也可以指定任意一个蒙层，例如一个字母A形状的蒙层。只有这个蒙层上的layer的内容部分才会被渲染。

### 离屏渲染(Offscreen Rendering)

离屏渲染能够被`Core Animation`或者你的应用程序自动触发。离屏渲染是指合成/渲染layer树到一个新的buffer(不在屏幕上)，然后再把这个buffer里的内容渲染到屏幕上。

当合成计算代价太高的时候你或许会强制开启离屏渲染，这是缓存合成的纹理的一种方法。如果你的渲染树(所有纹理和它们如何组合在一起)非常复杂，你可以强制使用离屏渲染来缓存这些合成好的layer，然后把缓存的内容合成显示到屏幕上。

如果你的app的页面有很多layer结合在一起的，GPU通常需要重新合成所有的layer。当使用离屏渲染的时候，GPU首先将这些层组合成一个基于新纹理的位图缓存，然后把新纹理显示到屏幕上。现在当这些layers要一起显示的时候，GPU使用这个位图缓存，从而减少工作量。值得注意的是，只有在这些layers不发生改变的时候才适用，一旦这些layers发生了改变，GPU需要重新生成位图缓存。你可以通过设置shouldRasterize来触发这个行为。

这里就需要有一个权衡，一方面，离屏渲染会让渲染变慢，GPU需要做一个额外的步骤来创建一个offscreen缓冲区，特别的，如果它不能重用这个位图，做的缓存就浪费精力了。如果缓存的位图能够重用的话，GPU就减负了。你必须测量GPU利用率和帧速率来确定离屏渲染是否对你有帮助。

离屏渲染也可能产生副作用。如果你直接或间接给一个layer加一个蒙层mask，Core Animation被强制使用离屏渲染来增加这个蒙层mask。这增加了GPU的负担。通常情况下layer本来是可以直接渲染到帧缓冲区（屏幕）。

Instruments的Core Animation工具有一个叫做`Color Offscreen-Rendered Yellow`的选项，它会把使用离屏缓冲区渲染的区域染成黄色(在模拟器上同样适用)。确保把`Color Hits Green and Misses Red`选项也勾上。绿色表示离屏缓冲区被重用，红色表示离屏缓冲区被重新创建。

通常情况下，你想要避免离屏渲染，因为代价太高了。直接把layers合成到帧缓冲区比先创建已经离屏缓冲区，渲染到里面，再把离屏缓冲区的内容渲染到帧缓冲区(屏幕)代价更小。后者有两次高代价的上下文切换：把上下文切换到离屏缓冲区，然后再把上下文切换到帧缓冲区。

因此，当你把`Color Offscreen-Rendered Yellow`选项打开，然后看到了黄色，就可以理解为一个警告，警告你有地方使用了离屏渲染了。但这并不一定是坏事，如果Core Animation能够重用离屏渲染的缓存，它可以提高性能。当layers没有改变时就可以重用。

还要注意的是，栅格化层(rasterized layers)的空间是有限的,Apple暗示栅格化层(rasterized layers)/离屏缓冲区的屏幕尺寸大约是屏幕的两倍。

如果你使用layer的方式导致了离屏渲染，你最好还是要尽量避免。使用蒙层mask、给layer谁知一个圆角半径或者使用阴影都会导致离屏渲染。

对于蒙层mask来说，有圆角半径，`clipsToBounds / masksToBounds`,这样的蒙层你或许可以简单的创建一个刚好包含这些的内容，例如一个刚好有圆角的图片。和以前一样，你需要权衡利弊。如果你想使用一个矩形的蒙层，你可以用`contentsRect`代替蒙层mask。

如果你把`shouldRasterize`设置为YES，记得将`rasterizationScale`设置为`contentsScale`。

### 更多关于合成(Compositing)

和往常一样，维基百科上有更多关于[alpha合成](https://en.wikipedia.org/wiki/Alpha_compositing)的数学方面的背景知识。我们将会在后续讨论[像素](https://www.objc.io/issues/3-views/moving-pixels-onto-the-screen/#pixels)的时候更深入的讨论如何在内存中表示红色、绿色、蓝色和alpha。

### OS X

如果你在开发OS X上的应用，你会发现上面提到的大多数的调试选项在另一个独立的叫做`Quartz Debug`的app上，不在Instruments里面。`Quartz Debug`是`Graphics Tools`的一部分，需要在[开发者网站](https://developer.apple.com/downloads/)上单独下载。

## Core Animation & OpenGL ES

顾名思义，Core Animation可以让屏幕上的内容产生动画效果。我们调过动画的部分，专注于绘图的部分。Core Animation让你可以非常高效的渲染，这就是为什么你可以使用Core Animation做每秒60帧的动画。

Core Animation的核心部分是一个抽象的OpenGL ES。简单地说，它让你可以使用OpenGL ES的强大功能而不用处理OpenGL ES的复杂性。上面我们说Compositing(合成)的时候，我们把layer(层)和texture(纹理)交替使用，他们不是一个东西，但是很相似。

Core Animation的layers可以有sublayers，因此最后你得到的是一个layer树。Core Animation会找出那些需要重绘的layers，然后调用OpenGL ES把这些layers合成到屏幕上。

例如，当你把一个layer的内容设置为一个`CGImageRef`，Core Animation就创建了一个OpenGL的纹理，确保图片中的位图上传到到对应的纹理中。又如，你重写了`-drawInContext`方法，Core Animation将会创建一个纹理并确保你调用的Core Graphics能变成纹理中的位图数据。一个layer的属性和`CALayer`子类会影响OpenGL渲染的效果，许多底层的OpenGL ES行为都被很好的封装成了易于理解的`CALayer`中的概念。

Core Animation一方面使用Core Graphics编排基于CPU的位图绘制，另一方面使用OpenGL ES编排基于CPU的位图绘制。因为Core Animation位于渲染流程中非常关键的位置，所以你如何使用Core Animation会很大程度上影响性能。

### CPU限制 VS. GPU限制

许多原件在你展示内容到屏幕上时都在工作，两个主要的硬件就是CPU和GPU。P和U两个字母代表processing unit，当你在屏幕上绘图的时候他们两个都在做处理，他们两个也都有资源限制。

为了达到60帧每秒的帧率，你必须确保CPU和GPU都不能超负荷工作。另外，即使帧率在60帧每秒，你也想把尽可能多的工作交给GPU。你想让CPU闲下来跑你的应用程序代码而不是忙于绘图。并且，在渲染方面GPU也比CPU更高效，可以降低系统的负荷和功耗。

因为绘图的性能决定于CPU和GPU，你需要找出是哪一个限制了你的绘图性能。如果你使用了所有的GPU资源，那么GPU就是限制你绘图性能的因素，你的绘图就是GPU限制的。相同的，如果你最大限度地使用了CPU，那么就是CPU限制的。

如果是GPU限制的，你需要给GPU减负，比如把更多的工作交给CPU来做。如果是CPU限制的，你需要给CPU减负。

为了区分到底是GPU限制的还是CPU限制的，可以使用`OpenGL ES Driver`工具。点击`i`按钮，然后点击`configure`按钮，确保` Device Utilization %`选中状态，当你把你的app跑起来的时候，就能够看到GPU的负荷量。如果这个值接近100%，说明你正在试图在GPU上做太多的工作。

更常见的是CPU做了太多的工作，`Time Profiler`能够忙你监控这种情况。

## Core Graphics/Quartz 2D

Quartz 2D更广为人知的是它包含的框架：Core Graphics。

Quartz 2D比我们这里提到的有更多的奇巧淫技。我们不讲它里面和PDF创建、渲染、转化、打印相关的部分，只需要知道打印和创建PDF和把位图画到屏幕上是一样的，因为都是基于Quartz 2D的。

让我们简单介绍一下Quartz 2D的主要概念。更多关于Quartz 2D的详细内容，请参考苹果的文档[Quartz 2D Programming Guide](https://developer.apple.com/library/mac/#documentation/GraphicsImaging/Conceptual/drawingwithquartz2d/Introduction/Introduction.html)。

请放心，当涉及到2D绘图时，Quartz 2D是非常强大的。Quartz 2D有基于路径的绘图、抗锯齿(anti-aliased)绘制、透明层、分辨率和设备独立性，来列举一些特性，这听起来令人生畏，因为它是一个低级的基于c的API。

不过，主要的概念还是比较简单的。为了简单实用这些API，UIKit和AppKit都把Quartz 2D封装的非常简单，即使是简单的C API，一旦你习惯了，它也是可以访问的。最后你会得到一个可以在Photoshop和Illustrator中完成大部分工作的绘图引擎。苹果以[股票app](https://developer.apple.com/videos/wwdc/2011/?id=129)为例讲了Quartz 2D的使用，因为股票图是一个使用Quartz 2D动态渲染的一个简单例子。

当你的app在做位图画图的时候，以这样或者那样的方式，都是基于Quartz 2D。也就是说，你绘图的CPU部分将会被Quartz 2D执行。尽管Quartz 2D可以做很多其他的事情，我们这里也只关注它位图绘图部分，也即是把结果绘制到一个包含RGBA数据的内存缓冲区上。

我们要画一个八边形(Octagon)。时候用UIKit我们要这么做：

```
UIBezierPath *path = [UIBezierPath bezierPath];
[path moveToPoint:CGPointMake(16.72, 7.22)];
[path addLineToPoint:CGPointMake(3.29, 20.83)];
[path addLineToPoint:CGPointMake(0.4, 18.05)];
[path addLineToPoint:CGPointMake(18.8, -0.47)];
[path addLineToPoint:CGPointMake(37.21, 18.05)];
[path addLineToPoint:CGPointMake(34.31, 20.83)];
[path addLineToPoint:CGPointMake(20.88, 7.22)];
[path addLineToPoint:CGPointMake(20.88, 42.18)];
[path addLineToPoint:CGPointMake(16.72, 42.18)];
[path addLineToPoint:CGPointMake(16.72, 7.22)];
[path closePath];
path.lineWidth = 1;
[[UIColor redColor] setStroke];
[path stroke];
```

对应的使用Core Graphics的代码类似，如下：

```
CGContextBeginPath(ctx);
CGContextMoveToPoint(ctx, 16.72, 7.22);
CGContextAddLineToPoint(ctx, 3.29, 20.83);
CGContextAddLineToPoint(ctx, 0.4, 18.05);
CGContextAddLineToPoint(ctx, 18.8, -0.47);
CGContextAddLineToPoint(ctx, 37.21, 18.05);
CGContextAddLineToPoint(ctx, 34.31, 20.83);
CGContextAddLineToPoint(ctx, 20.88, 7.22);
CGContextAddLineToPoint(ctx, 20.88, 42.18);
CGContextAddLineToPoint(ctx, 16.72, 42.18);
CGContextAddLineToPoint(ctx, 16.72, 7.22);
CGContextClosePath(ctx);
CGContextSetLineWidth(ctx, 1);
CGContextSetStrokeColorWithColor(ctx, [UIColor redColor].CGColor);
CGContextStrokePath(ctx);
```

现在的问题是，画图在哪里做的？这就是传说中的`CGContext`发挥作用的地方，我们传入的`ctx`参数就是这个context(上下文)。这个上下文定义了我们在哪里画图。如果你正在实现`CALayer`的`-drawInContext:`方法，我们就正在传递一个上下文，向这个上下文绘图就会绘图到这个layer的缓冲区。我们也可以自己创建一个上下文，也就是一个基于位图的上下文，例如`CGBitmapContextCreate()`，这个方法返回一个上下文，可以把它传递给`CGContext`函数画到那个上下文上。

注意，UIKit版本的代码并没有传递上下文参数到这个方法，因为使用UIKit或者AppKit时上下文是隐式的。UIKit管理了一个上下文的栈，UIKit的方法总是在栈顶绘图。你可以使用`UIGraphicsGetCurrentContext()`方法来获取这个上下文。使用`UIGraphicsPushContext()`方法push一个上下文到栈里面，使用`UIGraphicsPopContext()`使栈顶的上下文出栈。

最值得注意的是，UIKit有`UIGraphicsBeginImageContextWithOptions()`和`UIGraphicsEndImageContext()`两个非常方便的方法和Core Graphics的`CGBitmapContextCreate()`类似来创建一个位图上下文。混合使用UIKit和Core Graphics非常简单：

```
UIGraphicsBeginImageContextWithOptions(CGSizeMake(45, 45), YES, 2);
CGContextRef ctx = UIGraphicsGetCurrentContext();
CGContextBeginPath(ctx);
CGContextMoveToPoint(ctx, 16.72, 7.22);
CGContextAddLineToPoint(ctx, 3.29, 20.83);
...
CGContextStrokePath(ctx);
UIGraphicsEndImageContext();
```

或者反过来：

```
CGContextRef ctx = CGBitmapContextCreate(NULL, 90, 90, 8, 90 * 4, space, bitmapInfo);
CGContextScaleCTM(ctx, 0.5, 0.5);
UIGraphicsPushContext(ctx);
UIBezierPath *path = [UIBezierPath bezierPath];
[path moveToPoint:CGPointMake(16.72, 7.22)];
[path addLineToPoint:CGPointMake(3.29, 20.83)];
...
[path stroke];
UIGraphicsPopContext(ctx);
CGContextRelease(ctx);
```

你可以用Core Graphics做很多很Cool的事情，苹果的文档有无与伦比的保真度。我们不能深入地讲所有的细节，但是Core Graphics有一个图形模型(历史原因)和[ Adobe Illustrator](https://en.wikipedia.org/wiki/Adobe_Illustrator)和[ Adobe Photoshop ](https://en.wikipedia.org/wiki/Adobe_Photoshop)的工作机制很接近。这些工具的大多数概念都被翻译到了Core Graphics。毕竟它的来源是使用了[Display PostScript](https://en.wikipedia.org/wiki/Display_PostScript)的[NeXTSTEP](https://en.wikipedia.org/wiki/NextStep)。

### CGLayer

我们之前就说过，CGLayer可以用来加速相同元素的重复绘制。正如[戴夫·海登(Dave Hayden)](https://twitter.com/davehayden)所指出的，大街上的传言不再是事实。

## 像素(Pixels)

屏幕上的像素由红、绿、蓝三个颜色元素组成，因此位图数据有时候也指RGB数据。你可能想知道这个数据在内存中是如何组织的，事实是在内存中RGB位图数据有很多很多种表现形式。

我们一会儿会谈到压缩数据，这又是完全不同的。现在，让我们来看看红、绿、蓝每个颜色元素都有一个值的RGB位图数据，通常情况下我们还有第四个元素，透明度，最终我们会为每个像素得到四个独立的数据。

### 默认的像素排列

一个非常普遍的排列方式是**32bits-per-pixel (bpp)，8 bits-per-component (bpc)，最前面是alpha值**。在内存中的表现形式如下：

```
  A   R   G   B   A   R   G   B   A   R   G   B  
| pixel 0       | pixel 1       | pixel 2   
  0   1   2   3   4   5   6   7   8   9   10  11 ...
```

这种格式称为ARGB。每个像素使用4个字节(32bpp)，每个颜色单元是一个字节(8bpc)，每个像素有一个alpha值，放在RGB之前，最终红-绿-蓝颜色值和alpha值相乘。这里的相乘是只和红、绿、蓝颜色分别相乘。例如我们有一个橘色，它的RGB值按照8bpc模式显示分别是240，99，24，一个完全不透明的橘色像素的ARGB值为255，240，99，24，如果相同颜色值的像素，透明度为33%，那么像素的ARGB值就会是84，80，33，8。

另一种常见的格式是**32bpp**，**8bpc**，**alpha-none-skip-first**，就像是下面这样：

```
x   R   G   B   x   R   G   B   x   R   G   B  
| pixel 0       | pixel 1       | pixel 2   
  0   1   2   3   4   5   6   7   8   9   10  11 ...
```

这种格式称为xRGB。这样的像素没有任何alpha值(假定是完全不透明的)，但是内存分布是一样的。你会好奇为什么这种格式更加流行，因为如果没有alpha值，之前的一个字节就省下来了，这样就会节省25%的空间。事实证明，这种格式更容易被现代的cpu和成像算法所接受，因为单个像素与32位边界对齐。现代的CPU都不喜欢加载不是位对齐的数据，算法上也需要很多移位和掩码，尤其是当和之前的有alpha值的格式混合的时候。

处理RGB数据的时候，Core Graphics框架也支持吧alpha值放在最后面，有时候也称作RGBA和RGBx。


















