---
layout: article
title: "把像素显示到屏幕上(译)"
categories: iOS
excerpt: "如何把一个像素显示到屏幕上？有很多种方法，这些方法包含不同的framework和很多不同的函数和方法的组合。在这里，我们将讲述一些发生在幕后的事情。"
ads: true
share: false
image:
---

**By [Daniel Eggert](https://twitter.com/danielboedewadt)** 

[原文网址](https://www.objc.io/issues/3-views/moving-pixels-onto-the-screen/)

如何把一个像素显示到屏幕上？有很多种方法，这些方法包含不同的framework和很多不同的函数和方法的组合。在这里，我们将讲述一些发生在幕后的事情。我们希望这篇文章能帮你理解当你需要决定调试和修复性能问题的时候哪个API最好。这篇文章虽然是以iOS为例，但是大多数讲的内容也同时适用于OS X。

## 图形堆栈

当像素要显示到屏幕上时，许多事情在幕后悄悄进行着。但是一旦这些像素已经在屏幕上了，每个像素由三个元素组成：红、黄、蓝。三种单独的颜色单元以不同的强度叠加显示，组成一个特定颜色的像素。你的iPhone5，液晶显示屏有1,136*640 = 727,040个像素，也就是2,181,120个颜色单元。一个15英寸的retina屏幕的话，这个数字将会超过15500000。整儿图形堆栈一起动作来确保每个颜色单元显示正确的强度。当你滚动屏幕时，所有的这些鼠疫百万计的像素每秒要更新60次，这是一个巨大的工作量。

### 软件部分

简单的看，软件上的栈看起来就像这样：

![](https://www.objc.io/images/issue-3/pixels-software-stack@2x-1ae69f5a.png)

Display的上面是GPU（graphics processing unit）。GPU是一个专门为并行图形计算设计的高并发的处理单元。这样才能更新大量的像素点，并把他们显示在屏幕上。它的并行特性也使得它可以很有效地合成纹理。我们一会儿会更详细的讨论合成纹理。关键是GPU是专门设计的，因此在处理一些类似工作时更加高效，例如它处理图像时比cpu更快，消耗的能量更少。CPU有着广泛的用途，可以做很多不同的事情，但是例如合成纹理的话，还是GPU更快。

GPU驱动是一段可以直接和GPU对话的代码。它让不同类型的GPU对下一层OpenGL/OpenGL ES表现的更加统一。

OpenGL(Open Graphics Libiary) 是一个渲染2D或者3D图像的API。GPU是一个非常专业化的硬件，OpenGL和GPU非常紧密的合作来发挥GPU的计算特性和实现硬件加速渲染。对于很多人来说，OpenGL或许看起来非常底层，但是它在1992年发布时，就是第一个和GPU对话的主要的标准方法，这是一个巨大的进步，因为程序员再也不用为了各个GPU重写他们的应用。

在OpenGL上一层，有点儿分叉。在iOS系统上，几乎所有的事情都是通过Core Animation，在OS X上，Core Graphics 绕过Core Animation的情况非常常见。对于一些专门化的应用来说，尤其是游戏，app 可以直接和OpenGL/OpenGL ES对话。有点儿让人有点儿迷惑的是Core Animation 的部分渲染使用了 Core Graphics，像AVFoundation、Core Image和其他框架这样的框架可以将所有东西混合起来。

要记住一件事情，GPU是一个非常强大的进行图像处理的硬件，在显示像素上处于中心地位。它和CPU连接。在硬件层面，总线连接GPU和CPU，像OpenGL, Core Animation和 Core Graphics这些框架协调GPU和CPU之间的数据传输。为了让像素点显示在屏幕上，有些处理需要在CPU上进行，然后处理过的数据传输给GPU，GPU再做处理，最终像素点显示在屏幕上。

这个过程的每个部分都有自己的挑战的部分，在这个过程中也会有一些权衡。

### 硬件部分

![](https://www.objc.io/images/issue-3/pixels,%20hardware@2x-861825d9.png)

一个非常简单的挑战看起来像这样：GPU有一些由每一帧合成的纹理(位图)，每个纹理都占用VRAM(Vedio RAM)，因此GPU可以保留多少纹理是有限制的。即使GPU在合成纹理方面非常高效，但是总有一些更加复杂的合成任务，而GPU在16.7ms(1/60s)的时间内处理多少工作也是有限制的，不可能是无限的。

另一个挑战是把要处理的数据给到GPU。为了能让GPU访问到数据，需要把数据从RAM转移到VRAM里面，也就是把数据上传到GPU。这似乎微不足道，单对于大量要处理的纹理来说就会是非常耗时的操作。

最后，你的应用程序是在CPU上跑的，你会告诉CPU从你的bundle文件中加载一个PNG图片，并解压它，这些都发生在CPU上。当你想要显示解压的图片时，就需要上传到GPU。像显示文本这样平凡的事情对于CPU来说是非常复杂的任务，它要使Core Text 和 Core Graphics框架紧密结合来生成文本的位图，然后以纹理的格式上传给GPU去显示。当你在屏幕上拖动或移动这个文本，这个纹理可以重用，CPU只需要告诉GPU新的位置就可以了，因此GPU可以重用已经存在的纹理。CPU不需要重新渲染文本，也不需要重新上传位图。

以上说明了其中的一些复杂性。了解了这部分概述，我们将深入探讨其中的一些技术。

## 合成纹理

在图形世界中`合成`是一个术语，用来描述不同的位图是如何组合在一起来创建你在屏幕上看到的最终图像。从很多方面来说，显而易见，让你很容易忘记涉及的复杂性和计算。

让我们忽略一些更深奥的案例，假定所有屏幕上的内容都是纹理。纹理是一个RGBA值的矩形区域，也就是说，对于每个像素都有红、绿、蓝颜色值和透明度，在Core Animation里，这叫做`CALayer`。

在这个稍微简化的设置下，每个layer都是一个纹理，所有的这些纹理都是以某种方式相互叠加。对于屏幕上的每个像素来说，GPU需要弄清楚如何混合这些纹理得到这个像素最终的RGB值，这就是`合成`。

如果我们在屏幕上只有一个单一的纹理，并且和屏幕大小相等，每个屏幕上的像素对应纹理上的像素，纹理的像素最终成为屏幕的像素。

如果我们有第二个纹理放在第一个纹理的上面，GPU就不得不合成这个纹理到第一个上。我们假定两个纹理是像素对齐的，并且使用正常的混合模式，用下面的公式计算每个像素的颜色：

```
R = S + D * (1 - Sa)
```

结果颜色Result等于Source颜色(上层)加Destination颜色(下层)乘以1减去Source颜色的alpha值。

很明显，这里发生了很多事情，让我们再假定一下所有的纹理都是不透明的，也即是alpha=1，如果destination纹理是蓝色（RGB = 0，0，1），source纹理是红色（RGB = 1，0，0），因为Sa=1，所以`R = S`,结果颜色是红色，和你的期望值相符。

如果Source（上层）layer的透明度是50%，S的RGB值将会是（0.5，0，0），公式将会变成这样：

```
                       0.5   0               0.5
R = S + D * (1 - Sa) = 0   + 0 * (1 - 0.5) = 0
                       0     1               0.5
```

我们最终得到了一个RGB值(0.5,0,0.5)，一个饱和的紫色，这是你在把透明红色和蓝色背景混合在一起时的第一感觉。

目前为止我们只计算了一个纹理的单个像素和另一个纹理的单个像素的合成，而GPU则需要计算所有纹理的所有像素合成。正如你知道的，大多数的app有很多的layers，所有的纹理都需要合成到一起，即使GPU是一个高度优化的专门做这种事情的硬件，这也让GPU变得很忙。

### 不透明 VS. 透明度(Opaque vs. Transparent)

当Source纹理完全不透明时，生成的像素和Source纹理一样，这会节省GPU很多工作，因为它可以简单地复制Source纹理的像素值。但是，GPU无法分辨纹理中是否有像素不透明，只有作为程序员的你知道你向`CALayer`里放了什么。这就是`CALayer`有一个叫做`opaque`属性的原因，如果`opaque`是`YES`,GPU将会忽略下面的层直接从这个layer复制纹理，这节省了GPU大量的工作。在Instruments选项中`color blended layers`让你可以看到那些layers被标记为非不透明的，这样的layer需要混合。合成不透明layer消耗更少，因为计算的比较少。

因此，如果你知道你的layer是不透明的，把`opaque`设置成`YES`。如果你要加载一个没有alpha通道的图片然后显示到`UIImageView`，这是一个自动的过程。注意，一个没有alpha通道的图片和一个alpha为100%的图片有着巨大的差异，后者情况下，Core Animation需要假定或许有一些像素的alpha值不是100%，这就需要检验和计算。在`Finder`中，你可以通过`显示简介`中的`更多信息`中的`alpha通道`来判断图片是否包含alpha通道。

### 像素对齐和不对齐

到目前为止，我们已经研究了那些像素对齐的layers的显示，当所有的layer都是像素对齐的时候我们只需要相对简单的数学运算。无论何时GPU都要弄清楚屏幕上的像素应该是什么颜色，它只需要看在屏幕上的所有layers的像素，并合成他们，或者当最上层的layer是不透明时，只需要复制最上层的layer的纹理。

我们说一个layer是像素对齐的是指layer上所有的像素都和屏幕上的像素完美对齐。有以下两个原因会导致不是这种像素对齐的情况：第一、缩放，当一个纹理放大或者缩小的时候，纹理的像素点就无法和屏幕的像素点对齐；***第二、纹理的来源不是像素边界***。

在这两种情况下，GPU需要做额外的数学计算。它要把多个像素点混合计算出一个值用来合成。当像素对齐的时候，GPU做的工作就少很多。

再次，`Core Animation Instrument`有一个叫`color misaligned images`的选项，当像素不对齐时会告诉你是哪个layer实例。

### 蒙层(Mask)

一个layer有一个与之关联的蒙层，这个蒙层是一个在合成到下层layer之前应用到layer的像素上的alpha值的位图。当你设置layer的圆角半径时，实际上你是在那个layer上设置了一个蒙层。也可以指定任意一个蒙层，例如一个字母A形状的蒙层。只有这个蒙层上的layer的内容部分才会被渲染。

### 离屏渲染(Offscreen Rendering)

离屏渲染能够被`Core Animation`或者你的应用程序自动触发。离屏渲染是指合成/渲染layer树到一个新的buffer(不在屏幕上)，然后再把这个buffer里的内容渲染到屏幕上。

当合成计算代价太高的时候你或许会强制开启离屏渲染，这是缓存合成的纹理的一种方法。如果你的渲染树(所有纹理和它们如何组合在一起)非常复杂，你可以强制使用离屏渲染来缓存这些合成好的layer，然后把缓存的内容合成显示到屏幕上。

如果你的app的页面有很多layer结合在一起的，GPU通常需要重新合成所有的layer。当使用离屏渲染的时候，GPU首先将这些层组合成一个基于新纹理的位图缓存，然后把新纹理显示到屏幕上。现在当这些layers要一起显示的时候，GPU使用这个位图缓存，从而减少工作量。值得注意的是，只有在这些layers不发生改变的时候才适用，一旦这些layers发生了改变，GPU需要重新生成位图缓存。你可以通过设置shouldRasterize来触发这个行为。

这里就需要有一个权衡，一方面，离屏渲染会让渲染变慢，GPU需要做一个额外的步骤来创建一个offscreen缓冲区，特别的，如果它不能重用这个位图，做的缓存就浪费精力了。如果缓存的位图能够重用的话，GPU就减负了。你必须测量GPU利用率和帧速率来确定离屏渲染是否对你有帮助。

离屏渲染也可能产生副作用。如果你直接或间接给一个layer加一个蒙层mask，Core Animation被强制使用离屏渲染来增加这个蒙层mask。这增加了GPU的负担。通常情况下layer本来是可以直接渲染到帧缓冲区（屏幕）。

Instruments的Core Animation工具有一个叫做`Color Offscreen-Rendered Yellow`的选项，它会把使用离屏缓冲区渲染的区域染成黄色(在模拟器上同样适用)。确保把`Color Hits Green and Misses Red`选项也勾上。绿色表示离屏缓冲区被重用，红色表示离屏缓冲区被重新创建。

通常情况下，你想要避免离屏渲染，因为代价太高了。直接把layers合成到帧缓冲区比先创建已经离屏缓冲区，渲染到里面，再把离屏缓冲区的内容渲染到帧缓冲区(屏幕)代价更小。后者有两次高代价的上下文切换：把上下文切换到离屏缓冲区，然后再把上下文切换到帧缓冲区。

因此，当你把`Color Offscreen-Rendered Yellow`选项打开，然后看到了黄色，就可以理解为一个警告，警告你有地方使用了离屏渲染了。但这并不一定是坏事，如果Core Animation能够重用离屏渲染的缓存，它可以提高性能。当layers没有改变时就可以重用。

还要注意的是，栅格化层(rasterized layers)的空间是有限的,Apple暗示栅格化层(rasterized layers)/离屏缓冲区的屏幕尺寸大约是屏幕的两倍。

如果你使用layer的方式导致了离屏渲染，你最好还是要尽量避免。使用蒙层mask、给layer谁知一个圆角半径或者使用阴影都会导致离屏渲染。

对于蒙层mask来说，有圆角半径，`clipsToBounds / masksToBounds`,这样的蒙层你或许可以简单的创建一个刚好包含这些的内容，例如一个刚好有圆角的图片。和以前一样，你需要权衡利弊。如果你想使用一个矩形的蒙层，你可以用`contentsRect`代替蒙层mask。

如果你把`shouldRasterize`设置为YES，记得将`rasterizationScale`设置为`contentsScale`。

### 更多关于合成(Compositing)

和往常一样，维基百科上有更多关于[alpha合成](https://en.wikipedia.org/wiki/Alpha_compositing)的数学方面的背景知识。我们将会在后续讨论[像素](https://www.objc.io/issues/3-views/moving-pixels-onto-the-screen/#pixels)的时候更深入的讨论如何在内存中表示红色、绿色、蓝色和alpha。

### OS X

如果你在开发OS X上的应用，你会发现上面提到的大多数的调试选项在另一个独立的叫做`Quartz Debug`的app上，不在Instruments里面。`Quartz Debug`是`Graphics Tools`的一部分，需要在[开发者网站](https://developer.apple.com/downloads/)上单独下载。

## Core Animation & OpenGL ES

顾名思义，Core Animation可以让屏幕上的内容产生动画效果。我们调过动画的部分，专注于绘图的部分。Core Animation让你可以非常高效的渲染，这就是为什么你可以使用Core Animation做每秒60帧的动画。

Core Animation的核心部分是一个抽象的OpenGL ES。简单地说，它让你可以使用OpenGL ES的强大功能而不用处理OpenGL ES的复杂性。上面我们说Compositing(合成)的时候，我们把layer(层)和texture(纹理)交替使用，他们不是一个东西，但是很相似。

Core Animation的layers可以有sublayers，因此最后你得到的是一个layer树。Core Animation会找出那些需要重绘的layers，然后调用OpenGL ES把这些layers合成到屏幕上。

例如，当你把一个layer的内容设置为一个`CGImageRef`，Core Animation就创建了一个OpenGL的纹理，确保图片中的位图上传到到对应的纹理中。又如，你重写了`-drawInContext`方法，Core Animation将会创建一个纹理并确保你调用的Core Graphics能变成纹理中的位图数据。一个layer的属性和`CALayer`子类会影响OpenGL渲染的效果，许多底层的OpenGL ES行为都被很好的封装成了易于理解的`CALayer`中的概念。

Core Animation一方面使用Core Graphics编排基于CPU的位图绘制，另一方面使用OpenGL ES编排基于CPU的位图绘制。因为Core Animation位于渲染流程中非常关键的位置，所以你如何使用Core Animation会很大程度上影响性能。

### CPU限制 VS. GPU限制

许多原件在你展示内容到屏幕上时都在工作，两个主要的硬件就是CPU和GPU。P和U两个字母代表processing unit，当你在屏幕上绘图的时候他们两个都在做处理，他们两个也都有资源限制。

为了达到60帧每秒的帧率，你必须确保CPU和GPU都不能超负荷工作。另外，即使帧率在60帧每秒，你也想把尽可能多的工作交给GPU。你想让CPU闲下来跑你的应用程序代码而不是忙于绘图。并且，在渲染方面GPU也比CPU更高效，可以降低系统的负荷和功耗。

因为绘图的性能决定于CPU和GPU，你需要找出是哪一个限制了你的绘图性能。如果你使用了所有的GPU资源，那么GPU就是限制你绘图性能的因素，你的绘图就是GPU限制的。相同的，如果你最大限度地使用了CPU，那么就是CPU限制的。

如果是GPU限制的，你需要给GPU减负，比如把更多的工作交给CPU来做。如果是CPU限制的，你需要给CPU减负。

为了区分到底是GPU限制的还是CPU限制的，可以使用`OpenGL ES Driver`工具。点击`i`按钮，然后点击`configure`按钮，确保` Device Utilization %`选中状态，当你把你的app跑起来的时候，就能够看到GPU的负荷量。如果这个值接近100%，说明你正在试图在GPU上做太多的工作。

更常见的是CPU做了太多的工作，`Time Profiler`能够忙你监控这种情况。

## Core Graphics/Quartz 2D

Quartz 2D更广为人知的是它包含的框架：Core Graphics。

Quartz 2D比我们这里提到的有更多的奇巧淫技。我们不讲它里面和PDF创建、渲染、转化、打印相关的部分，只需要知道打印和创建PDF和把位图画到屏幕上是一样的，因为都是基于Quartz 2D的。

让我们简单介绍一下Quartz 2D的主要概念。更多关于Quartz 2D的详细内容，请参考苹果的文档[Quartz 2D Programming Guide](https://developer.apple.com/library/mac/#documentation/GraphicsImaging/Conceptual/drawingwithquartz2d/Introduction/Introduction.html)。

请放心，当涉及到2D绘图时，Quartz 2D是非常强大的。Quartz 2D有基于路径的绘图、抗锯齿(anti-aliased)绘制、透明层、分辨率和设备独立性，来列举一些特性，这听起来令人生畏，因为它是一个低级的基于c的API。

不过，主要的概念还是比较简单的。为了简单实用这些API，UIKit和AppKit都把Quartz 2D封装的非常简单，即使是简单的C API，一旦你习惯了，它也是可以访问的。最后你会得到一个可以在Photoshop和Illustrator中完成大部分工作的绘图引擎。苹果以[股票app](https://developer.apple.com/videos/wwdc/2011/?id=129)为例讲了Quartz 2D的使用，因为股票图是一个使用Quartz 2D动态渲染的一个简单例子。

当你的app在做位图画图的时候，以这样或者那样的方式，都是基于Quartz 2D。也就是说，你绘图的CPU部分将会被Quartz 2D执行。尽管Quartz 2D可以做很多其他的事情，我们这里也只关注它位图绘图部分，也即是把结果绘制到一个包含RGBA数据的内存缓冲区上。

我们要画一个八边形(Octagon)。时候用UIKit我们要这么做：

```
UIBezierPath *path = [UIBezierPath bezierPath];
[path moveToPoint:CGPointMake(16.72, 7.22)];
[path addLineToPoint:CGPointMake(3.29, 20.83)];
[path addLineToPoint:CGPointMake(0.4, 18.05)];
[path addLineToPoint:CGPointMake(18.8, -0.47)];
[path addLineToPoint:CGPointMake(37.21, 18.05)];
[path addLineToPoint:CGPointMake(34.31, 20.83)];
[path addLineToPoint:CGPointMake(20.88, 7.22)];
[path addLineToPoint:CGPointMake(20.88, 42.18)];
[path addLineToPoint:CGPointMake(16.72, 42.18)];
[path addLineToPoint:CGPointMake(16.72, 7.22)];
[path closePath];
path.lineWidth = 1;
[[UIColor redColor] setStroke];
[path stroke];
```

对应的使用Core Graphics的代码类似，如下：

```
CGContextBeginPath(ctx);
CGContextMoveToPoint(ctx, 16.72, 7.22);
CGContextAddLineToPoint(ctx, 3.29, 20.83);
CGContextAddLineToPoint(ctx, 0.4, 18.05);
CGContextAddLineToPoint(ctx, 18.8, -0.47);
CGContextAddLineToPoint(ctx, 37.21, 18.05);
CGContextAddLineToPoint(ctx, 34.31, 20.83);
CGContextAddLineToPoint(ctx, 20.88, 7.22);
CGContextAddLineToPoint(ctx, 20.88, 42.18);
CGContextAddLineToPoint(ctx, 16.72, 42.18);
CGContextAddLineToPoint(ctx, 16.72, 7.22);
CGContextClosePath(ctx);
CGContextSetLineWidth(ctx, 1);
CGContextSetStrokeColorWithColor(ctx, [UIColor redColor].CGColor);
CGContextStrokePath(ctx);
```

现在的问题是，画图在哪里做的？这就是传说中的`CGContext`发挥作用的地方，我们传入的`ctx`参数就是这个context(上下文)。这个上下文定义了我们在哪里画图。如果你正在实现`CALayer`的`-drawInContext:`方法，我们就正在传递一个上下文，向这个上下文绘图就会绘图到这个layer的缓冲区。我们也可以自己创建一个上下文，也就是一个基于位图的上下文，例如`CGBitmapContextCreate()`，这个方法返回一个上下文，可以把它传递给`CGContext`函数画到那个上下文上。

注意，UIKit版本的代码并没有传递上下文参数到这个方法，因为使用UIKit或者AppKit时上下文是隐式的。UIKit管理了一个上下文的栈，UIKit的方法总是在栈顶绘图。你可以使用`UIGraphicsGetCurrentContext()`方法来获取这个上下文。使用`UIGraphicsPushContext()`方法push一个上下文到栈里面，使用`UIGraphicsPopContext()`使栈顶的上下文出栈。

最值得注意的是，UIKit有`UIGraphicsBeginImageContextWithOptions()`和`UIGraphicsEndImageContext()`两个非常方便的方法和Core Graphics的`CGBitmapContextCreate()`类似来创建一个位图上下文。混合使用UIKit和Core Graphics非常简单：

```
UIGraphicsBeginImageContextWithOptions(CGSizeMake(45, 45), YES, 2);
CGContextRef ctx = UIGraphicsGetCurrentContext();
CGContextBeginPath(ctx);
CGContextMoveToPoint(ctx, 16.72, 7.22);
CGContextAddLineToPoint(ctx, 3.29, 20.83);
...
CGContextStrokePath(ctx);
UIGraphicsEndImageContext();
```

或者反过来：

```
CGContextRef ctx = CGBitmapContextCreate(NULL, 90, 90, 8, 90 * 4, space, bitmapInfo);
CGContextScaleCTM(ctx, 0.5, 0.5);
UIGraphicsPushContext(ctx);
UIBezierPath *path = [UIBezierPath bezierPath];
[path moveToPoint:CGPointMake(16.72, 7.22)];
[path addLineToPoint:CGPointMake(3.29, 20.83)];
...
[path stroke];
UIGraphicsPopContext(ctx);
CGContextRelease(ctx);
```

你可以用Core Graphics做很多很Cool的事情，苹果的文档有无与伦比的保真度。我们不能深入地讲所有的细节，但是Core Graphics有一个图形模型(历史原因)和[ Adobe Illustrator](https://en.wikipedia.org/wiki/Adobe_Illustrator)和[ Adobe Photoshop ](https://en.wikipedia.org/wiki/Adobe_Photoshop)的工作机制很接近。这些工具的大多数概念都被翻译到了Core Graphics。毕竟它的来源是使用了[Display PostScript](https://en.wikipedia.org/wiki/Display_PostScript)的[NeXTSTEP](https://en.wikipedia.org/wiki/NextStep)。

### CGLayer

我们之前就说过，CGLayer可以用来加速相同元素的重复绘制。正如[戴夫·海登(Dave Hayden)](https://twitter.com/davehayden)所指出的，大街上的传言不再是事实。

## 像素(Pixels)

屏幕上的像素由红、绿、蓝三个颜色元素组成，因此位图数据有时候也指RGB数据。你可能想知道这个数据在内存中是如何组织的，事实是在内存中RGB位图数据有很多很多种表现形式。

我们一会儿会谈到压缩数据，这又是完全不同的。现在，让我们来看看红、绿、蓝每个颜色元素都有一个值的RGB位图数据，通常情况下我们还有第四个元素，透明度，最终我们会为每个像素得到四个独立的数据。

### 默认的像素排列

一个非常普遍的排列方式是**32bits-per-pixel (bpp)，8 bits-per-component (bpc)，最前面是alpha值**。在内存中的表现形式如下：

```
  A   R   G   B   A   R   G   B   A   R   G   B  
| pixel 0       | pixel 1       | pixel 2   
  0   1   2   3   4   5   6   7   8   9   10  11 ...
```

这种格式称为ARGB。每个像素使用4个字节(32bpp)，每个颜色单元是一个字节(8bpc)，每个像素有一个alpha值，放在RGB之前，最终红-绿-蓝颜色值和alpha值相乘。这里的相乘是只和红、绿、蓝颜色分别相乘。例如我们有一个橘色，它的RGB值按照8bpc模式显示分别是240，99，24，一个完全不透明的橘色像素的ARGB值为255，240，99，24，如果相同颜色值的像素，透明度为33%，那么像素的ARGB值就会是84，80，33，8。

另一种常见的格式是**32bpp**，**8bpc**，**alpha-none-skip-first**，就像是下面这样：

```
  x   R   G   B   x   R   G   B   x   R   G   B  
| pixel 0       | pixel 1       | pixel 2   
  0   1   2   3   4   5   6   7   8   9   10  11 ...
```

这种格式称为xRGB。这样的像素没有任何alpha值(假定是完全不透明的)，但是内存分布是一样的。你会好奇为什么这种格式更加流行，因为如果没有alpha值，之前的一个字节就省下来了，这样就会节省25%的空间。事实证明，这种格式更容易被现代的cpu和成像算法所接受，因为单个像素与32位边界对齐。现代的CPU都不喜欢加载不是位对齐的数据，算法上也需要很多移位和掩码，尤其是当和之前的有alpha值的格式混合的时候。

处理RGB数据的时候，Core Graphics框架也支持吧alpha值放在最后面，有时候也称作RGBA和RGBx。

### 深奥的布局方式

大多数时候，处理位图数据的时候，我们使用Core Graphics / Quartz 2D来处理，它有一个非常具体的支持的格式列表。让我们先看下剩余的RGB格式：

另一种格式是**16bpp，5bpc，不带alpha值**。这种布局方式只占之前两种方式内存的50%，因为一个像素只要16位。如果你需要在内存或磁盘上存储(未压缩的)RGB数据，那么这将非常方便。但是这种格式每个像素的颜色单元只有5位，图像(特别是平滑渐变的)可能会有[色调分离(Posterization)](https://en.wikipedia.org/wiki/Posterization)出现。

还有一种格式是**64bpp，16bpc**最终**128bpp，32bpc，float-components**。这些每个像素使用8个字节或者16个字节的方式可以有更高的保真度，代价是更高的内存使用和更复杂的运算。

为了解决问题，Core Graphics还支持一些灰度和[CMYK](https://en.wikipedia.org/wiki/CMYK)的格式，还有一个字母的格式(用于掩码)。

### 平面数据

大多数的框架(包括Core Graphics)使用颜色单元(红、绿、蓝、透明度)混合的像素。但是有一些情况我们称之为**平面元素planar components**, 或者**component planes**，这时候每个颜色单元都在自己的内存区域内。对于RGB数据，我们有三个独立的内存区域，一个大的区域包含所有像素的红色元素，一个大区域包含所有像素的绿色元素，一个大区域包含所有像素的蓝色元素。

一些视频框架会在某些情况下使用平面数据。

### YCbCr

[YCbCr](https://en.wikipedia.org/wiki/YCbCr)在视频数据中比较普遍。它也包含三个元素(Y，Cb，Cr)能够表示颜色数据。但是(简单地说)它更接近人类视觉感知颜色的方式，人类视觉对两个色度元素Cb和Cr的保真度不太敏感，但是对于信号Y特别敏感。当数据采用YCbCr格式时，Cb和Cr元素的压缩比具有相同感知度的Y元素更困难。

因为这个原因，JPEG图片有时候也把像素数据从RGB转换为YCbCr。JPEG可以独立地压缩每一个颜色平面，当压缩基于YCbCr的平面时，Cb和Cr可以压缩比Y平面更多。

## 图片格式

在iOS或OS X上，图片主要是JPEG和PNG两种格式，我们详细看一下。

### JPEG

大家都知道[JPEG](https://en.wikipedia.org/wiki/JPEG)格式，从相机里出来的就是JPEG格式的图片。电脑上也是这么存储的。就连大妈也听说过JPEG。

出于某种原因，许多人认为JPEG文件只是另外一种格式化像素数据的方式，就像我们刚讲到的RGB像素在内存中的布局，但这与事实却相差很远。

把JPEG数据转换成像素数据是一个非常复杂的过程，即使是在一个非常长的周末，你也不能把它作为一个周末项目来完成。对于每个[颜色平面(color plane)](https://www.objc.io/issues/3-views/moving-pixels-onto-the-screen/#planar-data)，JPEG压缩使用基于[离散余弦变换](https://en.wikipedia.org/wiki/Discrete_cosine_transform)的算法将空间信息转换为频域。然后，使用Huffman编码的变体对这些信息进行量化、排序和打包。这样数据就从RGB转化成了YCbCr平面。当对JPEG进行解码时，所有这些都必须反向运行。

这就是为什么我们使用JPEG的图片创建一个`UIImage`并把它显示到屏幕上是时候会有延迟，因为CPU在忙于解压JPEG文件。如果你在每个`tableview cell`上解压JPEG文件，当你滚动的时候肯定不流畅。

那么为什么还要使用JPEG图片呢？答案是JPEG能够非常非常好的压缩图片。一个从iPhone 5照出的未经压缩的照片将近占24M内存，在默认的压缩设置下，你相机胶卷里的照片大约2M到3M。JPEG压缩的这么好，是因为它是有损压缩，它丢掉了哪些人眼识别度低的信息，这样做的话，它可以超越gzip这样的“正常”压缩算法的极限。但是这只对照片有很好的效果，因为JPEG依赖于一个事实，那就是照片中有很多对人类视觉来说不是很明显的东西。如果你对一个展示文字的网页进行截屏，JPEG就不是很有效了，压缩会更低，你或许会发现JPEG压缩已经改变了图片。

### PNG

PNG的发音是“ping”，和JPEG相反，它是无损压缩的。当你把图片保存成PNG，然后打开它，你会发现所有的像素数据都和原来一样。因为这个限制，PNG没法想JPEG那样很好的压缩图片。但是对于像app里面的按钮、图标来说，它恰好是效果很好的。另外，解码PNG数据也没有解码JPEG那么复杂。

在现实世界里，事情往往没有这么简单，实际上有许多不同的PNG格式。想了解更多细节的话请去维基百科。但是简单地说，PNG支持有alpha通道或者没有alpha通道的颜色(RGB)像素压缩。这也是另一个在app上效果很好的原因。

### 选择一个格式

当你在你的app中使用图片的时候，你需要选择一个格式，JPEG或者PNG。读写时候的压缩和解压器是被高度优化过的，在一定程度上甚至支持并行。随着苹果系统的免费升级，解压器也会持续优化。如果你想使用其他格式的图片，请注意着可能会影响你的app的性能，也有可能打开一个安全漏洞，因为图片解压是一个黑客喜欢攻击的目标。

已经有很多[优化PNG](https://duckduckgo.com/?q=%22optimizing%20PNG%22)的文章，如果你愿意的话可以自己去浏览器搜索。不过，很重要的一点是，要注意Xcode的优化PNG与大多数其他的优化引擎是完全不同的。

当Xcode优化这些PNG文件时，技术上讲，把这些PNG文件转化为[一个不再可用的PNG](https://developer.apple.com/library/ios/#qa/qa1681/_index.html)。但是iOS可以读取这些优化后的文件，事实上可以比正常的PNG解压有更快的速度。Xcode以这种方式改变PNG文件让iOS使用一个更高效的在普通PNG文件上无法使用的解压算法。值得注意的一点是它改变了像素的布局。我们之前在像素那部分已经讲到，有很多种方式表示RGB数据，如果图片的像素格式不是iOS的图像系统需要的，就需要对每个像素进行转化。因此我们可以选择正确的格式的图片，这样就不需要复杂的操作就可以有很高的性能。

再次强调一下，如果你能的话尽可能选择被称作[可变大小的图片](https://www.objc.io/issues/3-views/moving-pixels-onto-the-screen/#resizable-images)来开发你的app。你的文件会更小，因此需要从文件系统加载和解压的数据也更少。

## UIKit和像素

每一个UIKit的视图都有自己的`CALayer`。反过来，这个layer有一个像素位图(有点儿像图片)的备份存贮，这个备份存贮才是实际上被渲染到屏幕上的内容。

### 使用-drawRect:

如果你的类实现了`-drawRect:`方法，工作机制如下：

当你调用`-setNeedsDisplay`方法的时候，UIKit会调用在视图的layer的`-setNeedsDisplay`方法。这样就给这个layer设置了一个标志，标识需要展示，事实上什么工作都没做，所以你可以被允许调用`-setNeedsDisplay`多次。

下一步，当渲染系统准备好了之后，它调用在那个视图的layer`-display`方法。这时，这个layer设置它的后备存储，然后设置一个Core Graphics上下文(CGContextRef)，这个上下文由后备存储的内存区域备份，使用`CGContextRef `绘制的时候会直接访问那个内存区域。

当你在`-drawRect:`方法中使用UIKit的`UIRectFill()`或者`[UIBezierPath fill]`这样的绘图方法的时候，他们会使用这个上下文。这种方式，UIKit会把这个layer的后备存储的`CGContextRef`推入到图形上下文栈(graphics context stack)，也就是让这个上下文变成最上层的那个。这样`UIGraphicsGetCurrent()`方法就会返回这个上下文，绘图就会找到后备存贮进行绘制。如果你想直接使用Core Graphics的方法，你也可以通过`UIGraphicsGetCurrent()`方法得到这个上下文，把这个上下文作为Core Graphics的上下文传给它的方法。

从现在开始，layer的后备存贮会重复的渲染到屏幕上，直到视图的`-setNeedsDisplay`方法被重新调用，它使layer的后备存储更新。

### 不使用-drawRect:

当你使用`UIImageView`时，事情变得有点儿不同了。视图仍然有一个`CALayer`，但是layer不开辟一个后备存储，相反，它使用`CGImageRef`作为它的内容，渲染服务器把图片的二进制数据渲染到屏幕上。

这种情况下，没有绘制在进行。我们只是简单地把图片的位图数据传给`UIImageView`,`UIImageView`直接转给Core Animation，Core Animation直接转给渲染服务器。

### To -drawRect: or Not to -drawRect:

这听起来可能有点俗气，但是，最快的绘制就是你什么都不做的绘制，都交给系统来处理。

大多数时候，你能够侥把你的自定义视图和其他视图合成，或者从许多layer合成，或者把你的自定义视图和其他视图还有其他layer合成，更多信息请看克里斯的关于[自定义控制](https://www.objc.io/issues/3-views/custom-controls/)的文章。这是推荐的，因为UIKit的类都是经过高度优化的。

一个需要你自定义绘制代码的很好的例子是“finger painting”，这个app在[WWDC 2012’s session 506:优化2D绘图和动画性能](https://developer.apple.com/videos/wwdc/2012/?id=506)中有展示。

另一个使用自定义绘图的是iOS的股票app，股票的K线图是用Core Graphics画的。注意，尽管你自定义了绘图，你也不需要重写`-drawRect:`方法。有时候用`UIGraphicsBeginImageContextWithOptions()`或者`CGBitmapContextCreate()`方法来创建一个位图更有意义，拿到从两个方法的图片结果，把它设置为`CALayer`的内容，测试计算。我们下面会举一个例子。

### 纯色

我们来看个例子：

```
// Don't do this
- (void)drawRect:(CGRect)rect
{
    [[UIColor redColor] setFill];
    UIRectFill([self bounds]);
}
```

我们现在知道为什么这是不好的：这样使得Core Animation创建一个后备存储，我们让Core Graphics用一个纯红色填充这个后备存储，然后上传给GPU。

我们完全可以省去这些工作，不用实现`-drawRect:`方法，只需要简单的设置layer的`backgroundColor`。如果视图有一个`CAGradientLayer`作为它的layer，同样的技术也适用。

### 可变大小的图片

类似的，你可以使用可变大小的图片来减少图形系统的压力。我们假定程序中一个300 x 50点的按钮，那就是600 x 100 = 60K像素，或者60K x 4 = 240KB这么大的内存上传到GPU，然后消耗VRAM。如果我们使用可变大小的图片，可以使用一个54 x 12点的图片，这样就小于2.6k的像素或者10KB的内存。这样就更快了。

Core Animation可用使用`CALayer`的[contentsCenter](https://developer.apple.com/library/mac/documentation/graphicsimaging/reference/CALayer_class/Introduction/Introduction.html#//apple_ref/occ/instp/CALayer/contentsCenter)属性来重新定义图片的大小，大多数情况下，你会使用[-[UIImage resizableImageWithCapInsets:resizingMode:]](http://developer.apple.com/library/ios/documentation/UIKit/Reference/UIImage_Class/Reference/Reference.html#//apple_ref/occ/instm/UIImage/resizableImageWithCapInsets:resizingMode:)方法。

还要注意，在这个按钮第一次呈现之前，代替从文件系统读取然后解码一个60k像素PNG，解码一个更小的PNG图片更快。通过这种方式，你的应用程序在涉及到的所有步骤中都要少做一些工作，你的视图加载速度会更快。

### 并发绘制

[并发问题](https://www.objc.io/issues/2-concurrency/concurrency-apis-and-pitfalls/)。正如我们所知道的，UIKit的线程模型非常简单：你只能在主线程中使用UIKit的类。so，并发绘制是怎么回事儿呢？

如果你不得不实现`-drawRect:`方法，你需要绘制一些非普通的图，这就要花费时间。因为你想让你的动画平滑流畅，你将会在一个不是主线程的线程里尝试做这些事情。并发是复杂的，但是只要注意几点，并发绘制是容易实现的。

我们只能通过主线程而非其他线程往`CALayer`的后备存贮中绘制。但是我们能做的是绘制到一个没有连接的位图上下文。

正如我们在Core Graphics里面提到的，所有的Core Graphics绘制方法拿到一个上下文参数来指定绘制到哪个上下文中。相反地UIKit有一个当前上下文的概念。这个当前上下文是针对每个线程而言的。

为了实现并发绘制，我们要做以下事情。我们在其他线程创建一个image，一旦创建成功，我们就返回主线程把刚才创建的image设置为`UIImageView`的image。这个技术在[WWDC 2012 session 211](https://developer.apple.com/videos/wwdc/2012/?id=211)中有提到。

在你要绘图的地方添加一个方法：

```
- (UIImage *)renderInImageOfSize:(CGSize)size;
{
    UIGraphicsBeginImageContextWithOptions(size, NO, 0);

    // do drawing here

    UIImage *result = UIGraphicsGetImageFromCurrentImageContext();
    UIGraphicsEndImageContext();
    return result;
}
```

这个方法通过`UIGraphicsBeginImageContextWithOptions()`方法创建了一个指定大小的新位图`CGContextRef`，这个方法也使新的上下文变为当前的UIKit上下文，现在你可以像在`-drawRect:`方法里一样做你的绘制。然后当我们已经通过`UIGraphicsGetImageFromCurrentImageContext()`得到了这个上下文的位图数据--一个UIImage，我们结束这个上下文。

任何你在这个方法里的调用都是线程安全的非常重要，就像是你访问属性一样，他们应该是线程安全的。原因是你是从其他线程调用的这个方法。如果这个方法在你的试图类里面，这可能有点儿棘手。另一个选择是创建一个单独的渲染类，在这个类里面你设置所有的需要的属性，然后触发渲染图片。这样的话，你能够使用普通的`UIImageView`或者`UITableViewCell`。

注意，所有的UIKit的绘图API在其他线程上使用都是安全的。只要确保以`UIGraphicsBeginImageContextWithOptions()`开始，以`UIGraphicsEndIamgeContext()`结束，并在这之间操作。

你或许会像下面这样触发绘制代码：

```
UIImageView *view; // assume we have this
NSOperationQueue *renderQueue; // assume we have this
CGSize size = view.bounds.size;
[renderQueue addOperationWithBlock:^(){
    UIImage *image = [renderer renderInImageOfSize:size];
    [[NSOperationQueue mainQueue] addOperationWithBlock:^(){
        view.image = image;
    }];
}];
```

注意，我们把`view.image = image;`写在主队列中了，这是个非常重要的细节，你不能在其他队列里写。

和往常一样，并发绘制带来很多的复杂度，或许你不得不取消后台渲染。也或许你会在渲染队列里设置一个合理的最大数量并发操作数。

为了支持这些，最简单的是在`NSOperation`子类里实现`-renderInImageOfSize:`方法。

最后，必须指出，在`UITableViewCell`的内容上实现异步是非常棘手的。当你异步渲染好的时候，或许在这个时间点cell已经被重用了，这个时候的cell是用来做别的事情的，你却给它设置了老数据。












